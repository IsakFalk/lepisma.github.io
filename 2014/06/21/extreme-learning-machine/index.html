<!DOCTYPE html>
<html><head><meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<meta name="description" content="Personal Blog"/>
<meta name="author" content="Abhinav Tushar"/>
<meta name="keywords" content=""/>
<link rel="canonical" href="https://lepisma.github.io//2014/06/21/extreme-learning-machine/"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:type" content="article"/>
<meta property="og:title" content="Extreme Learning Machines with Julia"/>
<meta property="og:description" content="Personal Blog"/>
<meta property="og:url" content="https://lepisma.github.io//2014/06/21/extreme-learning-machine/"/>
<meta property="og:site_name" content="abhinav tushar"/>
<meta name="twitter:url" content="https://lepisma.github.io//2014/06/21/extreme-learning-machine/"/>
<meta name="twitter:title" content="Extreme Learning Machines with Julia"/>
<meta name="twitter:description" content="Personal Blog"/>
<meta itemprop="name" content="Extreme Learning Machines with Julia"/>
<meta itemprop="description" content="Personal Blog"/>
  <title>Extreme Learning Machines with Julia &#8211; abhinav tushar</title>
  <link rel="stylesheet" href="/assets/css/pace.css" type="text/css"/>
  <link rel="stylesheet" href="/css/blog.css" type="text/css"/>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css"/><link rel="apple-touch-icon-precomposed" sizes="57x57" href="/assets/images/favicons/apple-touch-icon-57x57.png"/>
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="/assets/images/favicons/apple-touch-icon-114x114.png"/>
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="/assets/images/favicons/apple-touch-icon-72x72.png"/>
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/images/favicons/apple-touch-icon-144x144.png"/>
<link rel="apple-touch-icon-precomposed" sizes="60x60" href="/assets/images/favicons/apple-touch-icon-60x60.png"/>
<link rel="apple-touch-icon-precomposed" sizes="120x120" href="/assets/images/favicons/apple-touch-icon-120x120.png"/>
<link rel="apple-touch-icon-precomposed" sizes="76x76" href="/assets/images/favicons/apple-touch-icon-76x76.png"/>
<link rel="apple-touch-icon-precomposed" sizes="152x152" href="/assets/images/favicons/apple-touch-icon-152x152.png"/>
<link rel="icon" type="image/png" href="/assets/images/favicons/favicon-196x196.png" sizes="196x196"/>
<link rel="icon" type="image/png" href="/assets/images/favicons/favicon-96x96.png" sizes="96x96"/>
<link rel="icon" type="image/png" href="/assets/images/favicons/favicon-32x32.png" sizes="32x32"/>
<link rel="icon" type="image/png" href="/assets/images/favicons/favicon-16x16.png" sizes="16x16"/>
<link rel="icon" type="image/png" href="/assets/images/favicons/favicon-128.png" sizes="128x128"/>
<meta name="application-name" content="&amp;nbsp;"/>
<meta name="msapplication-TileColor" content="#FFFFFF"/>
<meta name="msapplication-TileImage" content="/assets/images/favicons/mstile-144x144.png"/>
<meta name="msapplication-square70x70logo" content="/assets/images/favicons/mstile-70x70.png"/>
<meta name="msapplication-square150x150logo" content="/assets/images/favicons/mstile-150x150.png"/>
<meta name="msapplication-wide310x150logo" content="/assets/images/favicons/mstile-310x150.png"/>
<meta name="msapplication-square310x310logo" content="/assets/images/favicons/mstile-310x310.png"/><!-- Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,300,800" rel="stylesheet"/>
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css?family=Fira+Mono" rel="stylesheet"/>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
  <script src="/assets/js/pace.min.js"></script>
  <script src="/assets/js/jquery.zoomooz.min.js"></script>
</head>
  <body>
    <div class="site-wrap"><header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure"><span class="site-title"><a href="/"><img src="/assets/images/avatar32.png"/></a></span>
      <nav class="site-nav right"><a href="/">home</a><a href="/feed.xml">feed</a><a href="/archive">archive</a><a href="/about/">about</a></nav>
      <div class="clearfix"></div>
    </div>
  </div>
</header>
      <div class="post p2 p-responsive wrap" role="main">
        <div class="measure">

<div class="post-header mb2">
  <div class="post-meta small">Jun 21, 2014</div>
  <div class="post-meta small">16 minute read</div>
  <h1>Extreme Learning Machines with&nbsp;Julia</h1>
  <div class="post-tags"><a href="/archive/exploration/" rel="tag">exploration</a> <a href="/archive/ml/" rel="tag">ml</a></div>
</div>
<article class="post-content"><p class="post-intro">
Simple comparison of training time and performance of Extreme Learning Machines
and regular feed forward network using Julia language.
</p>
<!--more-->

<p><span class="dropcap">W</span>ith the advent of powerful computation techniques,
there has been increasing interest back in the good old neural networks as major
potential machine learning candidate after a long hiatus.</p>

<p>Modern and complex neural networks have come to the front, led by those dubbed
<a href="http://en.wikipedia.org/wiki/Deep_learning">Deep Learning Networks</a>. Deep
Learning algos are hot these days, thanks to the media. Just head over to
<a href="http://www.datatau.com/">datatau</a> and you will know what I mean <em>(btw I am not
saying that they aren’t worth the hype)</em>.</p>

<p>DL networks are basically networks with very deep (many) hidden layers in neural
nets. One major problem with them is (as expected) of speed. Deeper hidden
layers lead to deadly slow training time and the risk of overfitting. But recent
works by <a href="http://www.cs.toronto.edu/~hinton/">Hinton</a> and others in unsupervised
feature learning have given a hefty lift to deep learning.</p>

<h1 id="reservoirs">Reservoirs</h1>

<p>But, this post is not about Deep Learning. Its about a different concept. A
network that avoids the murky computation.</p>

<p>Recently, I came across the concept of <a href="http://en.wikipedia.org/wiki/Reservoir_computing">Reservoir
computing</a>, which (in a very
simple way) refers to a construct where the input are connected <em>randomly</em> to
higher level of abstraction (see it as a hidden layer of higher level) and
output can be tapped from those nodes and the training problem reduces to
calculating the connection (weights) of tapping nodes to output. This avoids the
major bottleneck, <em>iterative tuning of weights of input to higher level</em>.</p>

<p>Well, does this thing even work? <em>Yes, farely well!</em></p>

<p>There is a concept known as <em>Liquid State Machine</em>, and a relatively better
known <em>Echo State Network</em> which is used for training <a href="http://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural
Nets</a>. Both of them are
based on reservoir computing. On the lines of reservoir computing and very
similar in concept is the topic of this post, <em>Extreme Learning Machine</em>.</p>

<h1 id="extreme-learning">Extreme Learning</h1>

<p>Extreme learning machine (ELM) is a modification of single layer feedforward
network (SLFN) where learning is quite similar to the reservoir topic discussed
above.</p>

<p>Given below is a simple <em>SLFN</em> with 3 inputs and 1 output which computes the
weighted sum of hidden nodes.</p>

<figure><img class="zoomTarget" data-closeclick="true" src="/images/posts/elm/elm.jpg" /><figcaption>
    <p>ELM architecture</p>
  </figcaption></figure>

<p>After performing the tedious backpropagation ritual, we are left with the
following equation that predicts output from inputs</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>y</mi><mo>=</mo><mo>∑</mo><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">
y = \sum h_i
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.05em;"></span><span class="strut bottom" style="height:1.6000100000000002em;vertical-align:-0.55001em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mrel">=</span><span class="mop op-symbol large-op" style="position:relative;top:-0.000004999999999977245em;">∑</span><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span></span></p>

<p>Where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span> is the activation of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit">i</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> hidden neuron computed using</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo>(</mo><msub><mo>∑</mo><mi>j</mi></msub><mo>(</mo><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub><mo>×</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo><mo>+</mo><msub><mi>b</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">
h_i = f(\sum_j (w_{i, j} \times x_j) + b_i)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em;"></span><span class="strut bottom" style="height:2.463782em;vertical-align:-1.413777em;"></span><span class="base"><span class="mord"><span class="mord mathit">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8723309999999997em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em;"></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mbin">×</span><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p>

<p>Here, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord mathit" style="margin-right:0.10764em;">f</span></span></span></span> is the activation function like <em>sigmoid</em>,
<em>tanh</em> etc., <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord"><span class="mord mathit">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span></span></span></span> is the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>j</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">j^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span>
input, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{i, j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.716668em;vertical-align:-0.286108em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathit mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"></span></span></span></span></span></span></span></span> is the connection weight from
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>j</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">j^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:1.043548em;vertical-align:-0.19444em;"></span><span class="base"><span class="mord"><span class="mord mathit" style="margin-right:0.05724em;">j</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> input to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>i</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.849108em;"></span><span class="strut bottom" style="height:0.849108em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathit">i</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathit mtight">t</span><span class="mord mathit mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> hidden
neuron and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base"><span class="mord"><span class="mord mathit">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathit mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"></span></span></span></span></span></span></span></span> is the bias term.</p>

<p>In matrix notation, the process can be represented in the following form</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">Y</mi></mrow><mo>=</mo><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup><mo>×</mo><mrow><mi mathvariant="bold">H</mi></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{Y} = \mathbf{W}&#x27; \times \mathbf{H}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:0.8852220000000001em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em;">Y</span></span><span class="mrel">=</span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin">×</span><span class="mord"><span class="mord mathbf">H</span></span></span></span></span></span></p>

<p>Where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> is the matrix of weights from hidden
to output layer while <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">H</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">H</span></span></span></span></span> is the activation
matrix computed using</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">H</mi></mrow><mo>=</mo><mi>f</mi><mo>(</mo><mrow><mi mathvariant="bold">W</mi></mrow><mo>×</mo><mrow><mi mathvariant="bold">X</mi></mrow><mo>+</mo><mrow><mi mathvariant="bold">B</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">
\mathbf{H} = f(\mathbf{W} \times \mathbf{X} + \mathbf{B})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord mathbf">H</span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="mbin">×</span><span class="mord"><span class="mord mathbf">X</span></span><span class="mbin">+</span><span class="mord"><span class="mord mathbf">B</span></span><span class="mclose">)</span></span></span></span></span></p>

<p>with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">W</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span> is matrix of weights from input to
hidden layer.</p>

<p>Now, being different from usual SLFN, the ELM doesn’t tune <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">W</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span> using backprop or any other iterative method, instead
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">W</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span> is randomly generated. This gives us a pool
of higher level input abstractions in the hidden layer, out of which, ones
fitting to training data can be found by adjusting hidden to output weights by
solving a simple matrix equation.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">Y</mi></mrow><mo>=</mo><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup><mo>×</mo><mrow><mi mathvariant="bold">H</mi></mrow></mrow><annotation encoding="application/x-tex">
\mathbf{Y} = \mathbf{W}&#x27; \times \mathbf{H}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:0.8852220000000001em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em;">Y</span></span><span class="mrel">=</span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin">×</span><span class="mord"><span class="mord mathbf">H</span></span></span></span></span></span></p>

<p>Now, given a training data with <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">X</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">X</span></span></span></span></span> as input and
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">Y</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em;">Y</span></span></span></span></span> as output, the ELM training process takes
the following steps</p>

<ul>
  <li>Generate <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">W</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span></li>
  <li>Find <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">H</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.68611em;"></span><span class="strut bottom" style="height:0.68611em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord mathbf">H</span></span></span></span></span></li>
  <li>Solve <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mrow><mi mathvariant="bold">Y</mi></mrow><mo>=</mo><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup><mo>×</mo><mrow><mi mathvariant="bold">H</mi></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Y} = \mathbf{W}&#x27; \times \mathbf{H}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.835222em;vertical-align:-0.08333em;"></span><span class="base"><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em;">Y</span></span><span class="mrel">=</span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span><span class="mbin">×</span><span class="mord"><span class="mord mathbf">H</span></span></span></span></span> for
<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></li>
</ul>

<aside>
  <p>Mathematically inclined readers can refer to
<a href="http://www3.ntu.edu.sg/home/EGBHuang/pdf/ELM_IJCNN2004.PDF">this</a> paper. More
details about <em>ELMs</em> can be found <a href="http://www.ntu.edu.sg/home/egbhuang/">here</a></p>
</aside>

<p>The value of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup></mrow><annotation encoding="application/x-tex">\mathbf{W}&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.751892em;"></span><span class="strut bottom" style="height:0.751892em;vertical-align:0em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> can be calculated simply using
<em>psuedo (Moore-Penrose)</em> inverse which is usually available as a function that
goes by the name <em>pinv</em> most of the scientific computation environment including
Matlab and Python’s <code class="highlighter-rouge">numpy.linalg</code>.</p>

<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi mathvariant="bold">W</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mrow><mi mathvariant="bold">Y</mi></mrow><mo>×</mo><mi>p</mi><mi>i</mi><mi>n</mi><mi>v</mi><mo>(</mo><mrow><mi mathvariant="bold">H</mi></mrow><mo>)</mo></mrow><annotation encoding="application/x-tex">
\mathbf{W}&#x27; = \mathbf{Y} \times pinv(\mathbf{H})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:0.801892em;"></span><span class="strut bottom" style="height:1.051892em;vertical-align:-0.25em;"></span><span class="base"><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.801892em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">′</span></span></span></span></span></span></span></span></span><span class="mrel">=</span><span class="mord"><span class="mord mathbf" style="margin-right:0.02875em;">Y</span></span><span class="mbin">×</span><span class="mord mathit">p</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathbf">H</span></span><span class="mclose">)</span></span></span></span></span></p>

<p>This inverse can also be computed using regularized inverse for better
generalization.</p>

<h1 id="performance">Performance</h1>

<p>Lets test out the thing.</p>

<blockquote>
  <p>I happen to believe that we don’t need slow languages</p>
  <footer>
    <p>Jeff Bezanson. Co-creator, Julia</p>
  </footer>
</blockquote>

<p>Enough to make me try <a href="http://julialang.org/">julia</a>.</p>

<p>For python users (for anyone, almost), like me, switching is ridiculously easy
and fun. Although still in cradle, julia features nice set of basic libraries
for scientific computing. Kinds of
<a href="https://github.com/JuliaStats/DataFrames.jl">DataFrames</a>,
<a href="https://github.com/dcjones/Gadfly.jl">Gadfly</a> and
<a href="https://github.com/JuliaLang/IJulia.jl">IJulia</a> will make you feel at home,
whether you are coming from <em>R</em>, scientific <em>Python</em> or <em>Matlab / Octave</em>.</p>

<p>And what you get? <em>Speed</em>, raw and visible! Calling C or fortran from python
or R doesn’t feel great, especially if you can avoid that.</p>

<p>Coming back to testing. While trying out julia, I coded a simple
<a href="https://github.com/lepisma/ELM.jl">ELM library</a>. I will be using that, and for
comparison with regular NNs, I will be using the
<a href="https://github.com/EricChiang/ANN.jl">ANN library</a> by
<a href="https://twitter.com/erchiang">Eric Chiang</a>. In fact, this post is very much on
the lines of a great post by Eric on yhat
<a href="http://blog.yhathq.com/posts/julia-neural-networks.html">here</a>.</p>

<p>The problem I will be taking is of a two class classification using the banknote
authentication dataset. You can download the dataset and see its attributes
<a href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication">here</a>.</p>

<p>Starting off by installing required libraries</p>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">Pkg</span><span class="o">.</span><span class="n">clone</span><span class="x">(</span><span class="s">"git://github.com/lepisma/ELM.jl.git"</span><span class="x">);</span>
<span class="n">Pkg</span><span class="o">.</span><span class="n">clone</span><span class="x">(</span><span class="s">"git://github.com/EricChiang/ANN.jl.git"</span><span class="x">);</span>

<span class="k">import</span> <span class="n">ELM</span><span class="x">,</span> <span class="n">ANN</span><span class="x">;</span>
</code></pre>
</div>

<p>Since, both libraries have few functions with same names, so its better to use
<code class="highlighter-rouge">import</code> rather than <code class="highlighter-rouge">using</code>.</p>

<h2 id="reading-data">Reading data</h2>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">Pkg</span><span class="o">.</span><span class="n">add</span><span class="x">(</span><span class="s">"DataFrames"</span><span class="x">);</span>
<span class="n">using</span> <span class="n">DataFrames</span><span class="x">;</span>

<span class="c"># No column names here :(</span>
<span class="n">dat</span> <span class="o">=</span> <span class="n">readtable</span><span class="x">(</span><span class="s">"data_banknote_authentication.txt"</span><span class="x">,</span> <span class="n">header</span> <span class="o">=</span> <span class="n">false</span><span class="x">);</span>
<span class="n">head</span><span class="x">(</span><span class="n">dat</span><span class="x">)</span>
</code></pre>
</div>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="mi">6</span><span class="n">x5</span> <span class="n">DataFrame</span><span class="x">:</span>
             <span class="n">x1</span>      <span class="n">x2</span>      <span class="n">x3</span>       <span class="n">x4</span> <span class="n">x5</span>
<span class="x">[</span><span class="mi">1</span><span class="x">,]</span>     <span class="mf">3.6216</span>  <span class="mf">8.6661</span> <span class="o">-</span><span class="mf">2.8073</span> <span class="o">-</span><span class="mf">0.44699</span>  <span class="mi">0</span>
<span class="x">[</span><span class="mi">2</span><span class="x">,]</span>     <span class="mf">4.5459</span>  <span class="mf">8.1674</span> <span class="o">-</span><span class="mf">2.4586</span>  <span class="o">-</span><span class="mf">1.4621</span>  <span class="mi">0</span>
<span class="x">[</span><span class="mi">3</span><span class="x">,]</span>      <span class="mf">3.866</span> <span class="o">-</span><span class="mf">2.6383</span>  <span class="mf">1.9242</span>  <span class="mf">0.10645</span>  <span class="mi">0</span>
<span class="x">[</span><span class="mi">4</span><span class="x">,]</span>     <span class="mf">3.4566</span>  <span class="mf">9.5228</span> <span class="o">-</span><span class="mf">4.0112</span>  <span class="o">-</span><span class="mf">3.5944</span>  <span class="mi">0</span>
<span class="x">[</span><span class="mi">5</span><span class="x">,]</span>    <span class="mf">0.32924</span> <span class="o">-</span><span class="mf">4.4552</span>  <span class="mf">4.5718</span>  <span class="o">-</span><span class="mf">0.9888</span>  <span class="mi">0</span>
<span class="x">[</span><span class="mi">6</span><span class="x">,]</span>     <span class="mf">4.3684</span>  <span class="mf">9.6718</span> <span class="o">-</span><span class="mf">3.9606</span>  <span class="o">-</span><span class="mf">3.1625</span>  <span class="mi">0</span>
</code></pre>
</div>

<p>Last column is either 0 or 1 and tells us about the result of banknote
authentication.</p>

<h2 id="scaling-columns">Scaling columns</h2>

<p>Scaling all attributes to a similar scale makes sure that one attribute doesn’t
overshadow others.</p>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="c"># For all four columns</span>
<span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="x">:</span><span class="mi">4</span>
    <span class="c"># Subtracting the mean and dividing by standard deviation</span>
    <span class="n">dat</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="x">(</span><span class="n">dat</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">-</span> <span class="n">mean</span><span class="x">(</span><span class="n">dat</span><span class="x">[</span><span class="n">i</span><span class="x">]))</span> <span class="o">/</span> <span class="n">std</span><span class="x">(</span><span class="n">dat</span><span class="x">[</span><span class="n">i</span><span class="x">]);</span>
<span class="k">end</span>
</code></pre>
</div>

<p>Keeping 20% of data for testing</p>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">n_test</span> <span class="o">=</span> <span class="n">int</span><span class="x">(</span><span class="n">length</span><span class="x">(</span><span class="n">dat</span><span class="x">[</span><span class="k">end</span><span class="x">])</span> <span class="o">*</span> <span class="mf">0.2</span><span class="x">);</span>
<span class="n">train_rows</span> <span class="o">=</span> <span class="n">shuffle</span><span class="x">([</span><span class="mi">1</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">dat</span><span class="x">[</span><span class="k">end</span><span class="x">])]</span> <span class="o">.&gt;</span> <span class="n">n_test</span><span class="x">);</span>

<span class="n">dat_train</span><span class="x">,</span> <span class="n">dat_test</span> <span class="o">=</span> <span class="n">dat</span><span class="x">[</span><span class="n">train_rows</span><span class="x">,</span> <span class="x">:],</span> <span class="n">dat</span><span class="x">[</span><span class="o">!</span><span class="n">train_rows</span><span class="x">,</span> <span class="x">:];</span>
</code></pre>
</div>

<h2 id="training">Training</h2>

<p>Lets create the models for training.</p>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">ann</span> <span class="o">=</span> <span class="n">ANN</span><span class="o">.</span><span class="n">ArtificialNeuralNetwork</span><span class="x">(</span><span class="mi">10</span><span class="x">);</span>
<span class="c">#10 hidden neurons, single hidden layer</span>

<span class="n">elm</span> <span class="o">=</span> <span class="n">ELM</span><span class="o">.</span><span class="n">ExtremeLearningMachine</span><span class="x">(</span><span class="mi">10</span><span class="x">);</span>
<span class="c">#10 hidden neurons</span>
</code></pre>
</div>

<p>Although ELM is also given 10 neurons, but since ELMs select from a <em>pool</em>,
its better to give more options. But, whatever, the ultimate aim is to find the
difference in training time of both when they provide almost similar accuracy.</p>

<p>Like Matlab, you can time your code in julia using <code class="highlighter-rouge">tic()</code> and <code class="highlighter-rouge">toc()</code>
functions. Before that, let us make functions for calculating accuracy.(Both
libraries return values in different ways)</p>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="c"># For ANN</span>
<span class="k">function</span><span class="nf"> accu</span><span class="x">(</span><span class="n">model</span><span class="o">::</span><span class="n">ANN</span><span class="o">.</span><span class="n">ArtificialNeuralNetwork</span><span class="x">,</span>
                      <span class="n">x_test</span><span class="o">::</span><span class="n">Matrix</span><span class="x">{</span><span class="kt">Float64</span><span class="x">},</span>
                      <span class="n">y_test</span><span class="o">::</span><span class="n">Vector</span><span class="x">{</span><span class="kt">Int64</span><span class="x">})</span>

    <span class="n">outputs</span> <span class="o">=</span> <span class="n">ANN</span><span class="o">.</span><span class="n">predict</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">x_test</span><span class="x">);</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">Array</span><span class="x">(</span><span class="kt">Int64</span><span class="x">,</span> <span class="n">length</span><span class="x">(</span><span class="n">y_test</span><span class="x">));</span>

    <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="x">:</span><span class="n">length</span><span class="x">(</span><span class="n">y_test</span><span class="x">)</span>
        <span class="n">predictions</span><span class="x">[</span><span class="n">i</span><span class="x">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">classes</span><span class="x">[</span><span class="n">indmax</span><span class="x">(</span><span class="n">outputs</span><span class="x">[</span><span class="n">i</span><span class="x">,</span> <span class="x">:])];</span>
    <span class="k">end</span>

    <span class="n">mean</span><span class="x">(</span><span class="n">predictions</span> <span class="o">.==</span> <span class="n">y_test</span><span class="x">)</span>
<span class="k">end</span>

<span class="c"># For ELM</span>
<span class="k">function</span><span class="nf"> accu</span><span class="x">(</span><span class="n">model</span><span class="o">::</span><span class="n">ELM</span><span class="o">.</span><span class="n">ExtremeLearningMachine</span><span class="x">,</span>
                         <span class="n">data_test</span><span class="o">::</span><span class="n">DataFrame</span><span class="x">)</span>

    <span class="c">#ELM.jl supports DataFrames now !</span>

    <span class="n">predictions</span> <span class="o">=</span> <span class="n">ELM</span><span class="o">.</span><span class="n">predict</span><span class="x">(</span><span class="n">model</span><span class="x">,</span> <span class="n">data_test</span><span class="x">[</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">]);</span>
    <span class="n">mean</span><span class="x">(</span><span class="n">int</span><span class="x">(</span><span class="n">predictions</span><span class="x">)</span> <span class="o">.==</span> <span class="n">data_test</span><span class="x">[</span><span class="k">end</span><span class="x">])</span>
<span class="k">end</span>
</code></pre>
</div>

<p>Back to training. After a bit of experimentation, following approach (basically,
the choice of epochs in ANN) provides similar accuracy and a result that clearly
shows the difference.</p>

<p><em>A word of caution</em> : Since julia uses JIT compilation, it needs a bit of warm
up. So, the first call to functions doesn’t show the actual speed of julia.</p>

<h2 id="ann">ANN</h2>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">tic</span><span class="x">();</span> <span class="n">ANN</span><span class="o">.</span><span class="n">fit!</span><span class="x">(</span><span class="n">ann</span><span class="x">,</span> <span class="n">array</span><span class="x">(</span><span class="n">dat_train</span><span class="x">[</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">]),</span> <span class="n">array</span><span class="x">(</span><span class="n">dat_train</span><span class="x">[</span><span class="k">end</span><span class="x">]),</span> <span class="n">epochs</span> <span class="o">=</span>
<span class="mi">16</span><span class="x">,</span> <span class="n">lambda</span> <span class="o">=</span> <span class="mf">1e-5</span><span class="x">);</span> <span class="n">toc</span><span class="x">()</span>
<span class="n">accu</span><span class="x">(</span><span class="n">ann</span><span class="x">,</span> <span class="n">array</span><span class="x">(</span><span class="n">dat_test</span><span class="x">[</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">]),</span> <span class="n">array</span><span class="x">(</span><span class="n">dat_test</span><span class="x">[</span><span class="k">end</span><span class="x">]))</span>
</code></pre>
</div>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">elapsed</span> <span class="n">time</span><span class="x">:</span> <span class="mf">0.238218045</span> <span class="n">seconds</span>
<span class="mf">0.9708029197080292</span>
</code></pre>
</div>

<h2 id="elm">ELM</h2>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">tic</span><span class="x">();</span> <span class="n">ELM</span><span class="o">.</span><span class="n">fit!</span><span class="x">(</span><span class="n">elm</span><span class="x">,</span> <span class="n">dat_train</span><span class="x">[</span><span class="mi">1</span><span class="x">:</span><span class="k">end</span><span class="o">-</span><span class="mi">1</span><span class="x">],</span> <span class="n">dat_train</span><span class="x">[</span><span class="k">end</span><span class="x">]);</span> <span class="n">toc</span><span class="x">()</span>
<span class="n">accu</span><span class="x">(</span><span class="n">elm</span><span class="x">,</span> <span class="n">dat_test</span><span class="x">)</span>
</code></pre>
</div>

<div class="language-julia highlighter-rouge"><pre class="highlight"><code><span class="n">elapsed</span> <span class="n">time</span><span class="x">:</span> <span class="mf">0.003801902</span> <span class="n">seconds</span>
<span class="mf">0.9817518248175182</span>
</code></pre>
</div>

<h1 id="wut">WUT?</h1>

<p>Assuming both models give same accuracy, the training of <em>ELM</em> is around
<em>60x</em> faster than <em>ANN</em>! (Which kind of isn’t actually surprising since the
hidden connections are untouched).</p>

<div class="edits">
  <ul>
    <li>
      <p>As pointed out by Jeremy Gore, the current code throws error due to changes in
Julia version. The original post was tested and written for Julia v0.2. I will
update the post to meet the updated Julia standards after I get free from a
few things I am currently in.</p>
    </li>
    <li>
      <p>The library (and this post) is updated to work with newer Julia versions
(tested on v0.3.3) with added support for DataFrames.</p>
    </li>
    <li>
      <p>I thought to include my personal thoughts (which have also changed since I
first wrote the post) on ELMs since there are unbelievably large amount of
misconceptions popping everywhere on the internet.</p>

      <ul>
        <li>
          <p>As it is obvious, there is just one hidden layer and the whole ideas
centers around creating random projections of input to finally solve a
linear equation problem. This <em>can not be justified</em> as a solution for hard
and complex problems of the class currently tackled beautifully by deep
learning methods.</p>
        </li>
        <li>
          <p>Talking about the originality of the concept of <em>random projections</em>, I
personally am not a science historian (at least not right now) and would
prefer the reader to do his/her own research.</p>
        </li>
        <li>
          <p>The main thing that looked promising to me here was the idea that tapping
from random projections can solve <em>a class of problems</em>. This might not be
charming enough for anyone else, or even me at a different spot in
space-time, but anyways, I did a simple comparison and posted the stuff
here.</p>
        </li>
      </ul>
    </li>
  </ul>
</div>
</article></div>
      </div>
    </div>
    <footer class="footer"></footer>
  </body>
</html>