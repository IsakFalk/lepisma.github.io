#+TITLE: Notes
#+SETUPFILE: ../../assets/export.setup
#+TODO: TOREAD READING | READ

This contains notes for paper-ish documents that I read (notes for books in
general are [[pile:wiki:readings/books][here]]). I am now realizing the importance of summarizing what I read
so will be working on this page fairly regularly.

* Evolution/Complexity

** READ The causes of evolvability and their evolution
CLOSED: [2019-01-15 Tue 00:54]
:PROPERTIES:
:CUSTOM_ID: payne2018causes
:YEAR:     2018
:AUTHOR:   Payne, Joshua L and Wagner, Andreas
:END:

A very recent review on the subject with a bunch of experimental results. It
talks about three "major causes of evolvability":

1. Phenotype heterogeneity
2. Robustness
3. Adaptive landscapes

And

#+BEGIN_QUOTE
Whether they often evolve because they confer evolvability remains a
particularly challenging open question.
#+END_QUOTE

** READ Is evolvability evolvable?
CLOSED: [2019-01-06 Sun 16:01]
:PROPERTIES:
:CUSTOM_ID: pigliucci2008evolvability
:YEAR:     2008
:AUTHOR:   Pigliucci, Massimo
:END:

Barring for the various definitions of evolvability, we look at evolvability as
some sort of hyperparametrically derived quantity. These hyperparameters define
the phylogenic space the search will continue on /in the future/, therefore there
are the usual two arguments for evolution of evolability in the selection
setting:

1. as a side effect
2. targeted (sliding towards teleology)

Since this is a survey/opinion, there are not much technicalities here.

** READ Robustness and evolvability: a paradox resolved
CLOSED: [2018-12-27 Thu 10:38]
:PROPERTIES:
:CUSTOM_ID: wagner2007robustness
:YEAR:     2007
:AUTHOR:   Wagner, Andreas
:END:

Here we put up definitions for /robustness/ and /evolvability/ for sequences
(genotype) and structures (phenotype). The main conclusion says that these two
values are negatively correlated for genotype, but they support each other in
case of phenotype. There are a few quantitative results on the setting of RNA
sequences and the structures they form.

The question I am interested in is, how much can this be generalized to
arbitrary levels in arbitrary systems? The key property needed to get this
working is:

#+BEGIN_QUOTE
...even though structure robustness increases modestly with structure frequency,
this increase is much smaller than the vast increase in the number of different
structures accessible found near a much larger neutral network.
#+END_QUOTE

which gives

#+BEGIN_QUOTE
...the populations with the highly robust phenotype are more diverse, and this
increased diversity is much greater than the decreased diversity around any one
sequence.
#+END_QUOTE

** READ Robustness and evolvability
CLOSED: [2018-12-15 Sat 01:28]
:PROPERTIES:
:CUSTOM_ID: masel2010robustness
:YEAR:     2010
:AUTHOR:   Masel, Joanna and Trotter, Meredith V
:END:

A kind of review of the ideas behind evolutionary robustness. I got a few
pointers and terminology to follow from this paper.

** READ How learning can guide evolution
CLOSED: [2018-11-11 Sun 17:32] DEADLINE: <2018-11-11 Sun>
:PROPERTIES:
:CUSTOM_ID: hinton1987learning
:YEAR:     1987
:AUTHOR:   Hinton, Geoffrey E and Nowlan, Steven J
:END:

Simulation of a /minimalistic system/ for explaining the idea behind the searching
power of evolution + learning. Look [[https://egtheory.wordpress.com/2014/02/07/learning-guide-evolution/][here]] for an argument against the specific
example taken.

** READ Coevolution to the edge of chaos: coupled fitness landscapes, poised states, and coevolutionary avalanches
CLOSED: [2018-09-24 Mon 01:20]
:PROPERTIES:
:CUSTOM_ID: kauffman1991coevolution
:YEAR:     1991
:AUTHOR:   Kauffman, Stuart A and Johnsen, Sonke
:END:

This one uses the /NK model/ to experiment with coevolution. The main idea is that
you can couple one NK landscape to another using a factor similar to K, called
C, which defines how much the other affects this guy. Sounds like a reasonable
model to represent the essence of coevolving species. An important hint that we
get is that if a metadynamics is present to /select/ the value of K, then that
moves it to an attractor state where changes in the system cause avalanches
resembling the sandpile model from cite:bak1988self.

** READ Computation at the edge of chaos: phase transitions and emergent computation
 :PROPERTIES:
  :Custom_ID: langton1990computation
  :AUTHOR: Langton
  :JOURNAL: Physica D: Nonlinear Phenomena
  :YEAR: 1990
  :VOLUME: 42
  :PAGES: 12--37
 :END:

The question here focuses on how to get rules capable of computation in CAs.
Specifically, we are looking at /environments/ which characterize rules that
allow:

1. Storage of information
2. Transmission
3. Interaction between the above two

Intuitively, as the rule's output entropy increases, we move from a very simple
output (more /storage/) to output with randomness (more /transmission/). In between
these two, lies the region with the right amount of signal and noise with very
large transients and this is where most of the interesting events take place.

An interesting idea involves the definition of $\lambda$ parameter (that helps in
categorizing the rules) which is basically a discrete probability distribution
for the range of mapping function.

** Self-organized criticality
 :PROPERTIES:
  :Custom_ID: bak1988self
  :AUTHOR: Bak, Tang \& Wiesenfeld
  :JOURNAL: Physical review A
  :YEAR: 1988
  :VOLUME: 38
  :PAGES: 364
 :END:

** READ Revisiting the edge of chaos: Evolving cellular automata to perform computations
 :PROPERTIES:
  :Custom_ID: mitchell1993revisiting
  :AUTHOR: Mitchell, Hraber \& Crutchfield
  :JOURNAL: arXiv preprint adap-org/9303003
  :YEAR: 1993
 :END:

The edge of chaos idea is pretty popular and used to explain many phenomena. A
short article criticizing that is [[http://bactra.org/notebooks/edge-of-chaos.html][here]]. This is one of the papers that tried to
debunk (kind of) an experiment (cite:packard1988adaptation; this was in my
reading list for a long time) which claimed that evolving (in the GA sense) a CA
to solve computational problems gyrate it towards the edge of chaos.

It's pretty easy to see the issue since a solution to a /specific problem/ (they
took majority classification) is going to have a /specific \lambda/ and that's going to
be what that is, in spite of where the critical \lambda lies.

Other than that, this paper has some nice explanations and insights for the
results from GA. One neat trick that I haven't seen much (though I haven't seen
much) is of keeping the number of /elites/ high and changing the evaluation
function on each generation. This looks like a more practical way to use GAs in
evaluation over real data set. I also like the trick where you stop at a
variable number of generations to avoid getting a rule which gets the right
answer by alternating between 0s and 1s.

** READ Optimization by Self-Organized Criticality
 :PROPERTIES:
  :Custom_ID: hoffmann2018optimization
  :AUTHOR: Hoffmann \& Payton
  :JOURNAL: Scientific reports
  :YEAR: 2018
  :VOLUME: 8
  :PAGES: 2358
 :END:

I believe it is not /using/ SoC in the strict sense. The key is the generation of
test patterns. Using the sandpile model, we get a reasonable
exploration/exploitation trade offs. Also, two avalanches are less likely to
occur on overlapping patches (I am going by hunches on this so can be wrong) so
it also provides a more coordinate descent-ish behavior than the regular random
patch thing. Not sure if we can say that SoC is /specifically/ helping here.

There are two things. First is that this is better than the random approach
(consider /random patch/ since only that is fairly comparable). This probably
needs a lot more test cases or some theoretical justification.

Second is about the optimality of the sandpile approach. How about other non ~1/f~
distributions? I don't know which generating mechanisms can be employed to get
the test patterns but fishing around a bit tells me that this purity of
distribution is not that justified (consider for example the recent
cite:broido2018scale). The point being: if you fix an annealing schedule for
stimulated annealing based on some /natural/ observation, that doesn't:
1. create a parameter-less solver, and
2. justify the /natural/ observation to be the optimal

All said, I liked the thought of a random /object/ (?) generator which does better
than the regular approach in the general case. If there indeed is such a
generator, this could work as an off-the-shelf technique replacing uniform
random search.

** At the edge of chaos: Real-time computations and self-organized criticality in recurrent neural networks
 :PROPERTIES:
  :Custom_ID: bertschinger2005edge
  :AUTHOR: Bertschinger, Natschl\"ager \& Legenstein
  :JOURNAL: 
  :YEAR: 2005
  :PAGES: 145--152
 :END:

* AI/ML

** READ Comparison of grapheme-to-phoneme methods on large pronunciation dictionaries and LVCSR tasks
CLOSED: [2019-03-30 Sat 20:07]
:PROPERTIES:
:CUSTOM_ID: hahn2012comparison
:YEAR:     2012
:AUTHOR:   Hahn, Stefan and Vozila, Paul and Bisani, Maximilian
:END:

This is mostly a comparison of statistical g2p models. I think I have a general
idea now but looks like there is a lot more to see if I start looking into
individual references. A general thread along all these models was the use of a
certain alignment (grapheme to phoneme) algorithm to get what are called
/graphones/ and then train an ngrams-ish sequence model on them.

** READ Statistical language modeling for speech disfluencies
CLOSED: [2019-03-26 Tue 00:01]
:PROPERTIES:
:CUSTOM_ID: stolcke1996statistical
:YEAR:     1996
:AUTHOR:   Stolcke, Andreas and Shriberg, Elizabeth
:END:

Got here from srilm's disfluency (DF) LM. The idea is to have a /cleanup/ model
which models out certain common DFs, specifically filled pauses, repetitions and
deletions. Although there was not much gain, an interesting conclusion comes
with filled pauses where the DF model actually increased perplexity. The
argument being a filled pause, in most of the cases, linguistically breaks the
sentence and so the context behind it is not so useful for what follows.

Since the paper is old and also hints at a bunch of improvements in DF modeling,
I guess there might be a more recent reference around.

** READ SRILM-an extensible language modeling toolkit
CLOSED: [2019-03-26 Tue 00:01]
:PROPERTIES:
:CUSTOM_ID: stolcke2002srilm
:YEAR:     2002
:AUTHOR:   Stolcke, Andreas
:END:

This is an early document on SRILM's design and development. If you are looking
for something more in-depth, just download the current tarball.

** READ A bit of progress in language modeling
CLOSED: [2019-03-20 Wed 19:02]
:PROPERTIES:
:CUSTOM_ID: goodman2001bit
:YEAR:     2001
:AUTHOR:   Goodman, Joshua T
:END:

This has a lot of nice ideas and intuitions behind tricks employed in
statistical language models. I will just write out the general topics since it's
a long paper (~73 pages for the extended version):

- Skipping
- Clustering
- Caching
- Sentence Mixture Models

At a higher level we get to know about:
- ways of combining
- approaching analysis
- practical issues

** READ Streaming End-to-end Speech Recognition For Mobile Devices
CLOSED: [2019-03-19 Tue 00:12]
:PROPERTIES:
:CUSTOM_ID: he2018streaming
:YEAR:     2018
:AUTHOR:   He, Yanzhang and Sainath, Tara N and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and others
:END:

From Google's recent on-device character level Speech Recognition system. There
are a bunch of tricks used in the overall system other than the main model
itself. A few are:

- parameter quantization (required, of course, for fast computation on mobile
  device)
- data augmentation using tts for getting numbers, proper nouns etc. right
  (instead of doing fancy stuff on the model side)

** READ Rapidly building domain-specific entity-centric language models using semantic web knowledge sources
CLOSED: [2019-03-19 Tue 00:12]
:PROPERTIES:
:CUSTOM_ID: akbacak2014rapidly
:YEAR:     2014
:AUTHOR:   Akbacak, Murat and Hakkani-T{\"u}r, Dilek and Tur, Gokhan
:END:

This is focused on filtering search queries for creating language model. The
filtering that works out for them is to (after identifying a domain) go from
queries to clicked links then back to queries that went to those links. There
are a few other pieces involved but the general shape of narrowing is the same.

** The tradeoffs of large scale learning
:PROPERTIES:
:CUSTOM_ID: bottou2008tradeoffs
:YEAR:     2008
:AUTHOR:   Bottou, L{\'e}on and Bousquet, Olivier
:END:

** READ Abstract meaning representation for sembanking
CLOSED: [2019-03-10 Sun 22:43]
:PROPERTIES:
:CUSTOM_ID: banarescu2013abstract
:YEAR:     2013
:AUTHOR:   Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan
:END:

I found AMR while looking into a way of breaking from the usual intent/entity
based NLU. While they are [[https://nlpers.blogspot.com/2014/09/amr-not-semantics-but-close-maybe.html][not perfect]], the specification tells you about pieces
which should (in elaborate situations) be considered at least for practical
computational language understanding.

** READ Bootstrapping language models for dialogue systems
CLOSED: [2019-03-10 Sun 22:43]
:PROPERTIES:
:CUSTOM_ID: weilhammer2006bootstrapping
:YEAR:     2006
:AUTHOR:   Weilhammer, Karl and Stuttle, Matthew N and Young, Steve
:END:

This is quickly getting /domain specific/ LMs. The idea is to not do a lot of
manual (and perfect) text collection but start with simple grammars and get a
seed LM using the generated text. Then for more refinements, get a large LM and
do sentence selection on /in-the-wild/ data to get sentences with low value of
$PP_{seed} / PP_{large}$. These sentences and the /rejected/ ones then give two more LMs
which can then be interpolated based on a validation set.

Exact steps aside, the idea (other than SLMs on grammar generated data) is to do
some sort of /sentence selection/ to augment the seed LM.

** READ Developing Production-Level Conversational Interfaces with Shallow Semantic Parsing
CLOSED: [2019-01-15 Tue 01:10]
:PROPERTIES:
:CUSTOM_ID: raghuvanshi2018developing
:YEAR:     2018
:AUTHOR:   Raghuvanshi, Arushi and Carroll, Lucien and Raghunathan, Karthik
:END:

Doc on Mindmeld's NLU system.

** READ Neural text generation from structured data with application to the biography domain
CLOSED: [2019-01-05 Sat 23:22]
:PROPERTIES:
:CUSTOM_ID: lebret2016neural
:YEAR:     2016
:AUTHOR:   Lebret, R{\'e}mi and Grangier, David and Auli, Michael
:END:

From wikipedia info entry (a table) for a person, they generate biographical
sentences. The way to condition on the table while doing $P(w_i | c_{(i-1)})$ is
just indexing into (learnable) embeddings. I was looking for something more
insightful though.

** Generating exact lattices in the WFST framework
:PROPERTIES:
:CUSTOM_ID: povey2012generating
:YEAR:     2012
:AUTHOR:   Povey, Daniel and Hannemann, Mirko and Boulianne, Gilles and Burget, Luk{\'a}{\v{s}} and Ghoshal, Arnab and Janda, Milo{\v{s}} and Karafi{\'a}t, Martin and Kombrink, Stefan and Motl{\'\i}{\v{c}}ek, Petr and Qian, Yanmin and others
:END:

** READ Quantifying the value of pronunciation lexicons for keyword search in lowresource languages
CLOSED: [2019-01-27 Sun 23:55]
:PROPERTIES:
:CUSTOM_ID: chen2013quantifying
:YEAR:     2013
:AUTHOR:   Chen, Guoguo and Khudanpur, Sanjeev and Povey, Daniel and Trmal, Jan and Yarowsky, David and Yilmaz, Oguz
:END:

In a single line, while pronunciation dictionary augmentation doesn't help that
much in WER of an LVCSR (since the OOV rates are usually low), it helps a lot in
Keyword Search.

A few other things to note are the ways to generate pronunciation and two ways
to do KWS if you already have an LVCSR system. Not surprisingly, the proxy
keyword system doesn't work that well.

** READ State-of-the-art speech recognition with sequence-to-sequence models
CLOSED: [2018-11-06 Tue 20:52]
:PROPERTIES:
:CUSTOM_ID: chiu2018state
:YEAR:     2018
:AUTHOR:   Chiu, Chung-Cheng and Sainath, Tara N and Wu, Yonghui and Prabhavalkar, Rohit and Nguyen, Patrick and Chen, Zhifeng and Kannan, Anjuli and Weiss, Ron J and Rao, Kanishka and Gonina, Ekaterina and others
:END:

Bunch of improvements on top of the LAS architecture. It feels funny that even
in end-to-end systems, we still look for /modular presence/ of components like
Language Models. Maybe that helps in adding and justifying heuristics.

** Speech recognition with weighted finite-state transducers
:PROPERTIES:
:CUSTOM_ID: mohri2008speech
:YEAR:     2008
:AUTHOR:   Mohri, Mehryar and Pereira, Fernando and Riley, Michael
:END:

Partial notes:
1. Composition: Transitive-ness.
2. Determinization: Removing multiple transitions on same input.
3. Minimization: Compressing to the minimal, /equivalent/ automaton. Done by first
   weight pushing and then running the classical algorithm.

** Data programming: Creating large training sets, quickly
:PROPERTIES:
:CUSTOM_ID: ratner2016data
:YEAR:     2016
:AUTHOR:   Ratner, Alexander J and De Sa, Christopher M and Wu, Sen and Selsam, Daniel and R{\'e}, Christopher
:END:

** READ Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication
CLOSED: [2018-10-22 Mon 23:48]
:PROPERTIES:
:CUSTOM_ID: jaeger2004harnessing
:YEAR:     2004
:AUTHOR:   Jaeger, Herbert and Haas, Harald
:END:

This is the Echo State Network paper (probably not the original one but
sufficiently close). I found it to be a /little/ different than what I had earlier
thought about there being separate inputs and outputs.

** READ Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars
CLOSED: [2018-10-20 Sat 20:58] DEADLINE: <2018-10-16 Tue>
:PROPERTIES:
:CUSTOM_ID: zettlemoyer2012learning
:YEAR:     2012
:AUTHOR:   Zettlemoyer, Luke S and Collins, Michael
:END:

/Assuming the title clarifies the goal/, there are three basic components here:

1. A _parser_ which takes a sentence $S$, a set of categories $\Lambda$ and weights over
   features of the derivation (generated from parsing) $\theta$. This then generates
   logical forms ($L$) with certain probabilities.
2. Category _generator_ which takes $S$ and its expected logical form $L$ to
   generate the categories needed to parse it to that form.
3. An _estimator_ which, given the training set and a set of categories, updates
   $\theta$ to increase the score of the form getting parsed.

The interesting pieces are the representation of the logical form $L$ (using \lambda
calculus) and category generation and pruning. Although the generated categories
can be arbitrary, allowing for wrong grammars and such, I believe, it can be
made to work better in noisy settings if we generalize /parsing/ and (maybe) the
meaning of the structurally rigid categories like $S/NP$ using a few tricks.

** READ A very short introduction to CCG
CLOSED: [2018-10-16 Tue 02:21]
:PROPERTIES:
:CUSTOM_ID: steedman1996very
:YEAR:     1996
:AUTHOR:   Steedman, Mark
:END:

A lambda calculus formulation of /verb/ (function) acts in natural text. Not sure
if I can figure out exact advantages as compared to other approaches. This
definitely has more appeal to it because of the functional forms and the tooling
they pull in with themselves.

** READ Swoosh: a generic approach to entity resolution
CLOSED: [2018-10-07 Sun 20:23] SCHEDULED: <2018-10-06 Sat>
:PROPERTIES:
:CUSTOM_ID: benjelloun2009swoosh
:YEAR:     2009
:AUTHOR:   Benjelloun, Omar and Garcia-Molina, Hector and Menestrina, David and Su, Qi and Whang, Steven Euijong and Widom, Jennifer
:END:

The main products are optimal algorithms to do ER which minimize the number of
calls to the black box functions that actually perform the matching and merging.
To do this, we first formalize the ER problem using:

1. /Records/ and /features/ as the data structures
2. /Merging/ and /matching/ functions as the operations

Then we look for certain properties of a particular setting (mostly the effect
of merge and match functions). Based on whether a few of these are satisfied
(surprisingly trivial functions might not do what you expect of them), we can
reduce the number of calls to matching.

** READ How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation
CLOSED: [2018-10-13 Sat 17:45] SCHEDULED: <2018-10-06 Sat 15:00>
:PROPERTIES:
:CUSTOM_ID: liu2016not
:YEAR:     2016
:AUTHOR:   Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle
:END:

Other than the usuals, it has decent summaries of a few metrics used for
sentence similarity.

** READ Bringing machine learning and compositional semantics together
CLOSED: [2018-10-02 Tue 13:26]
:PROPERTIES:
:CUSTOM_ID: liang2015bringing
:YEAR:     2015
:AUTHOR:   Liang, Percy and Potts, Christopher
:END:

Got pointed to this while going through [[https://github.com/wcmac/sippycup][sippycup]]. This presents, in a very
pedagogical way, a simple framework for ranking semantic parses using supervised
learning. The important point is that this /framework/ can be applied to a lot of
problems in nlu involving different ways of structuring the logical forms and
features.

** A decision-theoretic generalization of on-line learning and an application to boosting
 :PROPERTIES:
  :Custom_ID: freund1997decision
  :AUTHOR: Freund \& Schapire
  :JOURNAL: Journal of computer and system sciences
  :YEAR: 1997
  :VOLUME: 55
  :PAGES: 119--139
 :END:

* Computation/Programming

** READ Practical type inference based on success typings
CLOSED: [2019-03-31 Sun 20:56]
:PROPERTIES:
:CUSTOM_ID: lindahl2006practical
:YEAR:     2006
:AUTHOR:   Lindahl, Tobias and Sagonas, Konstantinos
:END:

The general idea is to allow all programs that throw no runtime errors. This is
specially useful in languages which are philosophically dynamic. I like this
approach towards types since programming in a dynamic language involves dropping
a lot of so called writer's 'intention' here and there which does not adhere to
the static type philosophy.

Not sure if this is one of the firsts (the first for functional languages
according to the paper), but these days there are many mainstream dynamic
languages adopting such soft typing systems in some form.

** READ Dynamically typed languages
CLOSED: [2019-03-19 Tue 00:10]
:PROPERTIES:
:CUSTOM_ID: tratt2009dynamically
:YEAR:     2009
:AUTHOR:   Tratt, Laurence
:END:

A basic and exhaustive intro to dynamic typed languages. Good for beginners.

** READ Growing a language
CLOSED: [2019-03-12 Tue 11:11]
:PROPERTIES:
:CUSTOM_ID: steele1999growing
:YEAR:     1999
:AUTHOR:   Steele, Guy L
:END:

This is originally a talk, I read [[https://www.cs.virginia.edu/~evans/cs655/readings/steele.pdf][a pdf]] version. An interesting thing is the way
the talk itself is structured (its vocabulary mostly) exemplifying the same
/growth/ mechanism that Guy talks about in relation to languages.

** BlinkDB: queries with bounded errors and bounded response times on very large data
:PROPERTIES:
:CUSTOM_ID: agarwal2013blinkdb
:YEAR:     2013
:AUTHOR:   Agarwal, Sameer and Mozafari, Barzan and Panda, Aurojit and Milner, Henry and Madden, Samuel and Stoica, Ion
:END:

** Type systems as macros
:PROPERTIES:
:CUSTOM_ID: chang2017type
:YEAR:     2017
:AUTHOR:   Chang, Stephen and Knauth, Alex and Greenman, Ben
:END:

** Physics, topology, logic and computation: a Rosetta Stone
 :PROPERTIES:
  :Custom_ID: baez2010physics
  :AUTHOR: Baez \& Stay
  :YEAR: 2010
 :END:

** READ The Genuine Sieve of Eratosthenes
 :PROPERTIES:
  :Custom_ID: o2009genuine
  :AUTHOR: O'NEILL
  :JOURNAL: Journal of Functional Programming
  :YEAR: 2009
  :VOLUME: 19
  :PAGES: 95--106
 :END:

This talks about a functional implementation of Sieve of Eratosthenes.
Specifically it debunks the following incorrect implementation:

#+BEGIN_SRC haskell
primes = sieve [2..]
sieve (p : xs) = p : sieve [x | x <− xs, x `mod` p > 0]
#+END_SRC

Then we see correct functional implementations with neat tricks made possible
due to laziness of Haskell. Although slower, there is a list based
implementation by Bird mentioned in the /Epilogue/ which is pretty readable (and
elegant) and follows very closely the following description:

#+BEGIN_SRC
primes = [2, 3, ...] \ [[p², p²+p, ...] for p in primes]
#+END_SRC

** READ Why functional programming matters
 :PROPERTIES:
  :Custom_ID: hughes1989functional
  :AUTHOR: Hughes
  :JOURNAL: The computer journal
  :YEAR: 1989
  :VOLUME: 32
  :PAGES: 98--107
 :END:

This is a famous paper and I wanted to see what it focuses on. It's basically
about the following two properties and their effect on modularity in functional
programmings:

1. Higher order functions
2. Lazy evaluation

The examples are nice and make this is a good read for beginners. Though I
suspect there might be better, recent, articles on these topics now.

* Misc

** READ More is different
CLOSED: [2019-01-27 Sun 20:55]
:PROPERTIES:
:CUSTOM_ID: anderson1972more
:YEAR:     1972
:AUTHOR:   Anderson, Philip W and others
:END:

The basic idea is the following:

#+begin_quote
The main fallacy in this kind of thinking is that reductionist hypothesis does
not by any means imply a "constructionist" one.
#+end_quote

We are trying to understand that reductionist view is not going to explain
everything and that fundamental laws at the lowest level are not going to be /the
fundamental/ ones for the higher level ("Psychology is not applied biology...").
A littl0e hierarchy is also presented using examples where our movements across
levels results in /broken symmetry/:

- Crystallinity
- Functional structures
- /Regular systems/ with information like DNA
- Ordering in the time dimension for information processing etc.

#+begin_quote
So it is not true, as a recent article would have it, that we each should
"cultivate out own valley, and not attempt to build roads over the mountain
ranges ... between the sciences." Rather, we should recognize that such roads,
while often the quickest shortcut to another part of our own science, are not
visible from the viewpoint of one science alone.
#+end_quote

** READ Google's hybrid approach to research
CLOSED: [2018-10-30 Tue 02:34]
:PROPERTIES:
:CUSTOM_ID: spector2012google
:YEAR:     2012
:AUTHOR:   Spector, Alfred and Norvig, Peter and Petrov, Slav
:END:

Mostly about the people being /researchers/ and /developers/ and how it affects
various aspects of experiments.

** READ Machine learning: The high-interest credit card of technical debt
CLOSED: [2018-10-20 Sat 02:33]
:PROPERTIES:
:CUSTOM_ID: sculley2014machine
:YEAR:     2014
:AUTHOR:   Sculley, D and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael
:END:

** READ Better science through art
CLOSED: [2018-10-12 Fri 23:16] SCHEDULED: <2018-10-06 Sat>
:PROPERTIES:
:CUSTOM_ID: gabriel2010better
:YEAR:     2010
:AUTHOR:   Gabriel, Richard P and Sullivan, Kevin J
:END:

Here are the last few lines which cover what's common between Science and Art
and also summarize the document:

- Explore: wander / defamiliarize
- Discover: guess / abduce
- Understand: validate / ask—did you build the right thing?

** READ Lisp, Jazz, Aikido--Three Expressions of a Single Essence
 :PROPERTIES:
  :Custom_ID: verna2018lisp
  :AUTHOR: Verna
  :JOURNAL: arXiv preprint arXiv:1804.00485
  :YEAR: 2018
 :END:

Okay, this was up on /r/lisp, felt not that much effort to read so I gave it a
shot. There are three general aesthetic avenues that the author covers:

1. Conformation
2. Transgression
3. Unification

The general idea is about the similar interplay of these in all the 3 things
(Lisp, Jazz & Aikido) and how they end up being a source of pleasure and
enlightenment.

From whatever I have felt, /things/ that focus on an act itself (rather than
prioritizing the results) end up being like these (well, probably this is
obvious).

This paper is a quick read and is not overly philosophical. Maybe that's because
one of the focus is on tools that stay out of your way by staying /practical/ (you
can see this when the author talks about /Common Lisp/ specifically). Although I
must say that I know next to nothing about both /Jazz/ and /Aikido/ so might not
have really been able to connect all the pieces.


[[bibliography:./references.bib]]
